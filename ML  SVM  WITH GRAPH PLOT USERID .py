#!/usr/bin/env python
# coding: utf-8

# In[19]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# In[2]:


df=pd.read_csv("User_Data.csv")


# In[3]:


df


# In[5]:


from sklearn.preprocessing import StandardScaler
ss=StandardScaler()


# **The code snippet imports the `StandardScaler` class from scikit-learn and creates an instance named `ss`. This is used for standardizing data features.

# In[6]:


df[["Age"]]=ss.fit_transform(df[["Age"]])
df[["EstimatedSalary"]]=ss.fit_transform(df[["EstimatedSalary"]])


# **This code standardizes the "Age" and "EstimatedSalary" columns in the DataFrame, which means it scales these columns to have a mean of 0 and a standard deviation of 1. Standardization helps make different features comparable by ensuring they have the same scale, which can be beneficial for various machine learning algorithms. It's done using the `StandardScaler` object `ss` from scikit-learn.

# In[7]:


x=df.iloc[:,2:-1].values
y=df.iloc[:,-1].values


# In[10]:


from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=0)


# In[23]:


from sklearn.svm import SVC
svc=SVC(kernel='linear')
svc.fit(x_train,y_train)


# **This code snippet imports the `SVC` (Support Vector Classification) class from scikit-learn, creates an instance of `SVC` with a linear kernel, and then fits (trains) the classifier on the training data `x_train` and corresponding labels `y_train`. This is used for building a linear Support Vector Machine (SVM) classifier.

# # Plotting a Decison Boundary /Linear Line

# In[24]:


plt.figure(figsize=(10, 8))
# Plotting our two-features-space
sns.scatterplot(x=x_train[:, 0], 
                y=x_train[:, 1], 
                hue=y_train, 
                s=8);
# Constructing a hyperplane using a formula.
w = svc.coef_[0]           # w consists of 2 elements
b = svc.intercept_[0]      # b consists of 1 element
x_points = np.linspace(-1, 1)    # generating x-points from -1 to 1
y_points = -(w[0] / w[1]) * x_points - b / w[1]  # getting corresponding y-points
# Plotting a red hyperplane
plt.plot(x_points, y_points, c='r')  


# **This code snippet plots a scatterplot of two features using Seaborn, colors the points by class, and overlays a red hyperplane generated by an SVM classifier.

# # Plotting SVM Maximum Mragin

# In[25]:


plt.figure(figsize=(10, 8))
# Plotting our two-features-space
sns.scatterplot(x=x_train[:, 0], 
                y=x_train[:, 1], 
                hue=y_train, 
                s=8);
# Constructing a hyperplane using a formula.
w = svc.coef_[0]           # w consists of 2 elements
b = svc.intercept_[0]      # b consists of 1 element
x_points = np.linspace(-1, 1)    # generating x-points from -1 to 1
y_points = -(w[0] / w[1]) * x_points - b / w[1]  # getting corresponding y-points
# Plotting a red hyperplane
plt.plot(x_points, y_points, c='r');
# Encircle support vectors
plt.scatter(svc.support_vectors_[:, 0],
            svc.support_vectors_[:, 1], 
            s=50, 
            facecolors='none', 
            edgecolors='k', 
            alpha=.5);
# Step 2 (unit-vector):
w_hat = svc.coef_[0] / (np.sqrt(np.sum(svc.coef_[0] ** 2)))
# Step 3 (margin):
margin = 1 / np.sqrt(np.sum(svc.coef_[0] ** 2))
# Step 4 (calculate points of the margin lines):
decision_boundary_points = np.array(list(zip(x_points, y_points)))
points_of_line_above = decision_boundary_points + w_hat * margin
points_of_line_below = decision_boundary_points - w_hat * margin
# Plot margin lines
# Blue margin line above
plt.plot(points_of_line_above[:, 0], 
         points_of_line_above[:, 1], 
         'b--', 
         linewidth=2)
# Green margin line below
plt.plot(points_of_line_below[:, 0], 
         points_of_line_below[:, 1], 
         'g--',
         linewidth=2)


# In[ ]:


# **This code snippet does the following:

# 1. Sets up a figure for plotting with a specified size (10x8).

# 2. Plots a scatterplot of two features using Seaborn, where:
#    - `x_train[:, 0]` and `x_train[:, 1]` are the x and y coordinates from the training data.
#    - `hue=y_train` colors the points by class.
#    - `s=8` sets the size of the points.

# 3. Constructs a hyperplane using SVM classifier coefficients (`w` and `b`) and plots it as a red line.

# 4. Encircles support vectors with black circles.

# 5. Calculates a unit vector (`w_hat`) and margin for the hyperplane.

# 6. Computes points of the margin lines above and below the hyperplane.

# 7. Plots the margin lines:
#    - Blue dashed line for the margin above.
#    - Green dashed line for the margin below.

# In summary, this code visualizes an SVM classifier's decision boundary, margin, and support vectors in a 2D feature space.**


# # Prediction

# In[13]:


y_pred=svc.predict(x_test)
y_pred


# # Confusion Matrix

# In[15]:


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm=confusion_matrix(y_test,y_pred,labels=svc.classes_)
cm


# In[ ]:





# In[16]:


from sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Precision:",precision_score(y_test,y_pred))
print("Recall:",recall_score(y_test,y_pred))
print("Classification_report:",classification_report(y_test,y_pred))


# # Visualizing the training set result:

# In[26]:


from matplotlib.colors import ListedColormap  
x_set, y_set = x_train, y_train  
x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),  
np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  
plt.contourf(x1, x2, svc.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),  
alpha = 0.75, cmap = ListedColormap(('red', 'green')))  
plt.xlim(x1.min(), x1.max())  
plt.ylim(x2.min(), x2.max())  
for i, j in enumerate(np.unique(y_set)):  
    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],  
        c = ListedColormap(('red', 'green'))(i), label = j)  
plt.title('SVM classifier (Training set)')  
plt.xlabel('Age')  
plt.ylabel('Estimated Salary')  
plt.legend()  
plt.show()  


# # Visualizing the test set result:

# In[27]:


#Visulaizing the test set result  
from matplotlib.colors import ListedColormap  
x_set, y_set = x_test, y_test  
x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),  
np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  
plt.contourf(x1, x2, svc.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),  
alpha = 0.75, cmap = ListedColormap(('red','green' )))  
plt.xlim(x1.min(), x1.max())  
plt.ylim(x2.min(), x2.max())  
for i, j in enumerate(nm.unique(y_set)):  
    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],  
        c = ListedColormap(('red', 'green'))(i), label = j)  
plt.title('SVM classifier (Test set)')  
plt.xlabel('Age')  
plt.ylabel('Estimated Salary')  
plt.legend()  
plt.show()  


# # SVM GRIDSEARCH CV 

# In[30]:


from sklearn.model_selection import GridSearchCV
parameters = [{'C' : [0.25, 0.50, 0.75, 1], 'kernel' : ['linear']},
              {'C' : [0.25, 0.50, 0.75, 1], 'kernel' : ['rbf'], 'gamma' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},
              {'C' : [0.25, 0.50, 0.75, 1], 'kernel' : ['sigmoid'], 'gamma' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
grid_search = GridSearchCV(estimator = svc,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search.fit(x_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print('Best Accuracy : {:.2f}%'.format(best_accuracy*100))
print('Best Parameters : ', best_parameters)


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




